\section{Applicazioni lineari}
\subsection{Definizione: applicazione lineare (trasformazione lineare o homomorfism of vector space)}
Un'applicazione lineare è un'applicazione $T:V\rightarrow W$ di spazi vettoriali su $K$ tale che $\forall v,v_1,v_2\in V$, $\forall\lambda\in K$ si ha:
\begin{enumerate}
	\item $T(v_1+v_2)=T(v_1)+T(v_2)$ ($T$ è additiva)
	\item $T(\lambda\cdot v)=\lambda\cdot T(v)$ ($T$ è omogenea)
\end{enumerate}
(additiva+omogenea = è lineare).
\\In altre parole $T$ conserva le operazioni di spazio vettoriale.

\subsection{Proposizione}
Punti $1+2\Leftrightarrow 3:\; \forall\lambda\mu\in K,\forall v_1,v_2\in V$
\[T(\lambda v_1+\mu v_2)=\lambda T(v_1)+\mu T(v_2)\]
$T$ manda combinazioni lineari in combinazioni lineari (con gli stessi scalari) dei trasformati.

\subsection{Definizione}
Fissata $A\in \mathcal{M}_{mn}(\mathbb{R}):$ 
$A=\begin{bmatrix}a_{11}&...&a_{1_n}\\&...&\\a_{m1}&...&a_{mn}\end{bmatrix}$, sia $L_A:\mathbb{R}^n\rightarrow\mathbb{R}^m$ definita da:
\[X\begin{bmatrix}x_1\\...\\x_n\end{bmatrix}\mapsto L_A(x):=AX=\begin{bmatrix} a_{11}x_1+a_{12}x_2+...+a_{1n}x_n\\...\\a_{m_1}x_1+a_{m_2}x_2+...+a_{mn}x_n\end{bmatrix}\in\mathbb{R}^n\]
La trasformazione $L_A$ è la moltiplicazione (a destra) per $A$:
\[L_A:X\mapsto AX\]
ad ogni matrice $A$ corrisponde una trasformazione lineare $L_A$.

\textit{Per esercizio: dimostrare che $L_A:\mathbb{R}^n\rightarrow\mathbb{R}^m$ è lineare.
\begin{itemize}
	\item $L_A(X+Y)=L_A(X)+L_A(Y)$? (dimostrazione su appunti)
	\item $L_A(\lambda X)=\lambda L_A(X)?$
\end{itemize}}

\subsection{Definizione}
Data una trasformazione lineare $T:V\rightarrow W$, restano determinati due sottoinsiemi:
\begin{itemize}
	\item Il nucleo di $T$: $KerT$
	\[KerT=\{v\in V: T(v)=0_W\}\subseteq V\]
	\item L'immagine di $T$: $ImT$
	\[ImT=\{T(v):v\in V\}\subseteq W\]
\end{itemize}

\subsection{Proposizione (risolvere per esercizio)}
\begin{enumerate}
	\item $KerT\leq V$
	\item $ImT\leq W$
	\item $T$ suriettiva $\Leftrightarrow ImT=W$
	\item $T$ iniettiva $\Leftrightarrow KerT=\{0\}$
\end{enumerate}

\subsection{Osservazione (!)}
Calcolare il nucleo di un'applicazione lineare corrisponde a risolvere un sistema omogeneo.

\subsection{Proposizione}
Un'applicazione lineare $T:V\rightarrow W$ è definita univocamente (ovvero è determinata) quando si conoscono le immagini di $T$ sui vettori $\{v_1,...,v_n\}$ di una base di $V$, se $dimV=n$: ovvero basta conoscere
\(T(v_1),...,T(v_n)\in W\) per conoscere tutta la trasformazione lineare $T$.
\\
\\\textbf{Dimostrazione}
Se conosco $T$ su $v_1,...,v_n$ allora $v$ si scrive come $v=\alpha_1v_1+...+\alpha_nv_n$ (perchè $\mathcal{B}=\{v_1,...,v_n\}$ genera $v$).
$T$ è lineare:
\[T(v)=T(\alpha_1v_1+...+\alpha_nv_n)=\alpha_1T(v_1)+...+\alpha_nT(v_n)\]
Questo determina univocamente $T$:
\\\textit{Devo dimostrare che se esiste un'altra applicazione lineare $S\neq T$ ma che coincide con $T$ sulla base, allora ho una contraddizione.}
Sia $S$ un'altra applicazione $S:V\rightarrow W$ con $T(v_i)=S(v_i)\;\forall i=1,...,n$ allora
\[S(v)=\alpha_1S(v_1)+...+\alpha_nS(v_n)=\alpha_1T(v_1)+...+\alpha_nT(v_n)=T(v)\]
\\
\\$T$ è determinata completamente dai suoi valori su una base $B$ di $V$.

\subsection{Corollario} 
Due applicazioni lineari coincidono $\Leftrightarrow$ coincidono sui vettori di una base.

\subsection{Corollario}
Se $\mathcal{B}=(v_1,...,v_n)$ base ordinata di $V$ e se $T:V\rightarrow W$ lineare, allora $ImT=Span(\{T(v_1),...,T(v_n)\})$: cioè i vettori immagine della base di $V$ generano l'immagine della trasformazione.

In particolare, se $T=L_A$, trasformazione lineare associata ad $A=\begin{bmatrix}a_{11}&..&\\&...&\\&..&a_{mn}\end{bmatrix}$ 
\[T:\mathbb{R}^n\rightarrow\mathbb{R}^m\]
\[X\rightarrow AX=T(x)\]
allora
\[ImT=Im(L_A)=Span(T(e_1),...,T(e_n))\]
\[=Span(A\cdot e_1,A\cdot e_2,...,A\cdot e_n)\]
dove
\[A\cdot e_i=A\cdot\begin{bmatrix}0\\\vdots\\i\\\vdots\\0\end{bmatrix}=A^{(i)}\]
(Quindi questo punto del corollario ci dice che $Im(L_A)=Span(Colonne\;di\;A)$

\begin{center}
$dimIm(L_A)=$ dimensione spazio generato dalle colonne
\\=numero dei pivot di una ridotta scala equivalente ad $A$)
\end{center}

\subsection{Definizione: rango trasformazione lineare}
\begin{enumerate}
	\item Il rango di una trasformazione lineare $T:V\rightarrow W$ è $rg(T)=dim_KIm(T)$.
	\item Il rango di $L_A$ è la dimensione dell'immagine di $L_A$, che è il rango della matrice $A$, $ImL_A=\{AX:X\in\mathbb{R}^n\}$.
\end{enumerate}

\subsection{Teorema della dimensione}
Sia $T:V\rightarrow W$ trasformazione lineare. Allora:
\[dimV=dim_KKerT+dim_KImT\]
Sia $\{u_1,...,u_s\}\subseteq V$ una base di $KerT$
\begin{itemize}
	\item Generano $KerT$
	\item Sono indipendenti
	\item $T(u_1)=...=T(u_s)=0$
\end{itemize}

Per il teorema del completamento: completo $\{u_1,...,u_s\}$ ad una base di $V$, quindi sia $\{u_1,...,u_s,w_1,...,w_{n-s}\}$ base di $V$. Si candidano a base di $W$ i vettori $\{w_1,...,w_{n-s}\}$.
\\Dobbiamo mostrare che $t=\{T(w_1),...,T(w_{n-s})\}$ è una base di $ImT$, cioè che $rgT=n-s=dimV-dimKerT$

\begin{enumerate}
	\item $t$ genera $Im(T)$: sappieamo (dal corollario) che l'immagine $ImT$ è lo span di $(T(u_1),T(u_2),...,T(u_s),T(w_1),...,T(w_{n-s}))=Span(0_w,T(w_1),...,T(w_{n-s}))$
	\item Vediamo che $T(w_1),...,T(w_{n-s})$ sono indipendenti.
	\\Supponiamo $\alpha_1,...,\alpha_{n-s}\in\mathbb{R}$ tali che: 
	\[\alpha_1T(w_1)+...+\alpha_{n-s}T(w_{n-s})=0\]
	\[\Leftrightarrow T(\alpha_1w_1+...+\alpha_{n-s}w_{n-s})=0\;\;(T\;e'\;lineare)\]
	\[\Leftrightarrow \alpha_1w_1+...+\alpha_{n-s}w_{n-s}\in KerT\]
	\[\Rightarrow \beta_1,...,\beta_s:\;\alpha_1w_1+...+\alpha_{n-s}w_{n-s}=\beta_1u_1+...+\beta_su_s\]
	\[\alpha_1w_1+...+\alpha_{n-s}w_{n-s}-\beta_1u_1-...-\beta_su_s=0\]
	ed è una combinazione lineare dei vettori di una base di $V$ che dà $0$
	\[\Rightarrow\alpha_1=\alpha_2=...=\alpha_{n-s}=-\beta_1=...=-\beta_s=0\]
\end{enumerate}

\subsection{Osservazione}
Se $v_1,...,v_n$ sono i vettori colonna di $\mathbb{R}^n$; sia $A=\begin{bmatrix}v_1&v_2&\dots&v_n\end{bmatrix}$ la matrice $m\times n$ formata dai vettori.
\\Allora una combinazione lineare delle colonne $v_1,...,v_n$ con gli scalari $\alpha_1,...,\alpha_n$ ($\alpha_1v_1+...+\alpha_nv_n$) è la stessa cosa di:
\[A_{(m\times n)}\cdot\begin{bmatrix}\alpha_1\\\cdots\\\alpha_n\end{bmatrix}\in\mathbb{R}^m\]

\subsection{Proposizione}
\begin{enumerate}[label=\alph*)]
	\item $S,T:V\rightarrow W$, $S,T$ lineari, $V,W$ spazi vettoriali allora:
	$S+T,\;\lambda\cdot S$ sono lineari.
	\\Dove $(S+T)(v):=S(v)+T(v):\forall v\in V\;\;(S+T:V\rightarrow W)$
	\\e dove $(\lambda S)(v):=\lambda S(v)\;\;(\lambda S:V\rightarrow W)$

	\item Siano $S:U\rightarrow V,\;T:V\rightarrow W$ allora è possibile definire la composta $T\circ S:U\rightarrow W$, e $T\circ S$ è lineare.
\end{enumerate}
\textbf{Dimostrazione punto b) (la a) per esercizio)}
\\Da dimostrare: $T\circ S$ è lineare $\Leftrightarrow T\circ S(\alpha v_1+\beta v_2)=\alpha(T\circ S)(v_1)+\beta(T\circ S)(v_2)$
\[(T\circ S)(\alpha v_1+\beta v_2)=T(S(\alpha v_1+\beta v_2))=\]
\[T(\alpha S(v_1)+\beta S(v_2))=\;\;\text{(S è lineare)}\]
\[\alpha T(S(v_1))+\beta T(S(v_2))=\;\;\text{(T è lineare)}\]
\[\alpha (T\circ S)(v_1)+\beta(T\circ S)(v_2)\]
\[\Rightarrow T\circ S\text{ è lineare}\]

\subsection{Conseguenze della proposizione}
\begin{enumerate}[label=\alph*)]
	\item L'insieme $\mathcal{L}(V,W)=\{T:V\rightarrow W, T \text{ lineare}\}$ è uno spazio vettoriale
	\item $\mathcal{L}(V,V)$ l'insieme delle trasformazioni lineari da $V$ in se
	\[T:V\rightarrow V\;\;\;\;\;T\circ S\]
	\[S:V\rightarrow V\;\;\;\;\;S\circ T\]
	$(\mathcal{L}(V,V),+,\cdot)$
	\begin{itemize}
		\item è un anello non commutativo
		\item unitario $id:V\rightarrow V$, $T\cdot id=id\cdot T=T$ (elemento neutro è l'unità del prodotto)
		\item $0_V:V\rightarrow V\;(v\mapsto 0_V)$ trasformazione lineare nulla (elemeno neutro della somma)
	\end{itemize}
	\item Se in $\mathcal{L}(V,V)$ ci restringiamo a guardare le trasformazioni invertibili:
	\\$(\{T:V\rightarrow V:T\text{ lineare e invertibile}\},\cdot)$ è un gruppo commutativo.
\end{enumerate}

\subsection{Osservazione: iniettività, suriettività}
Sia $T:V\rightarrow W$ lineare e $dimV=dimW$, allora:
\begin{itemize}
	\item $T\text{ invertibile}\Leftrightarrow T\text{ iniettiva}$
	\item $T\text{ invertibile}\Leftrightarrow T\text{ suriettiva}$
\end{itemize}

\subsection{Osservazione: isomorfismo}
$V$ e $W$ sono isomorfi (\textbf{notazione}: $V\cong W)\Leftrightarrow\exists\;T:V\rightarrow W$ con $T$ biunivoca. 

\subsection{Esempio isomorfismo}
$V$ dimensione finita $n$; $B=(v_1,...,v_n)$ base ordinata.
\[f_B:V\rightarrow\mathbb{R}^n\]
\[v\mapsto f_B(v)=\begin{bmatrix}x_1\\\vdots\\x_n\end{bmatrix}\text{ dove }v=x_1v_1+...+x_nv_n\]
$f_B(v)$ sono le coordinate di $v$ in base $B$. Si ha che $f_B$ è un isomorfismo di $V$ in $\mathbb{R}^n$.

L'isomorfismo $V\xrightarrow{f_B}\mathbb{R}^n$ non è canonico, cioè dipende dalla scelta di una base.

\textbf{Conseguenza importante:} tutti gli spazi vettoriali di dimensione $n$ su $K$ sono tutti isomorfi a $K^n$.

\subsection{Teorema}
\[\mathcal{L}(\mathbb{R}^n,\mathbb{R}^m)\cong\mathcal{M}_{m,n}(\mathbb{R})^{\rightarrow[m\cdot n]}\]
\textbf{Dimostrazione:} abbiamo visto già come associare ad una matrice $A_{m\times n}$ un'applicazione lineare di $\mathbb{R}^n$ in $\mathbb{R}^m$.
\\$A\mapsto L_A$ la trasformazione lineare da $\mathbb{R}^n$ in $\mathbb{R}^m$ che agisce per moltiplicazione.
\[L_A:\mathbb{R}^n\rightarrow\mathbb{R}^m\]
\[X\mapsto AX\]

\[L:Matrici\;m\times n\rightarrow \mathcal{L}(\mathbb{R}^n,\mathbb{R}^m)\]
\[A\mapsto L(A)=L_A\]
\begin{itemize}
	\item $L$ è biunivoca? C'è l'inversa?
	\item $L$ è lineare? \textit{Fare da soli}
\end{itemize}
\textit{Inizio dimostrazione primo punto}:
\[L:\mathcal{M}_{m,n}(\mathbb{R})\rightarrow\mathcal{L}(\mathbb{R}^n,\mathbb{R}^m)\]
\[A\leftarrow T\]
Ora
\[T:\mathbb{R}^n\rightarrow\mathbb{R}^m\]
\[e_1\mapsto T(e_1)\in\mathbb{R}^m\]
\[\vdots\]
\[e_n\mapsto T(e_n)\in\mathbb{R}^m\]
Sia $A:=\begin{bmatrix}T(e_1) & T(e_2) & \dots & T(e_n)\end{bmatrix}$ è una matrice $m\times n$.
\\Devo mostrare che $L_A=T (\textit{completare da soli!})$

\subsection{Osservazione}
Si può dimostrare che $L_A\cdot L_B=L_{A\cdot B}$

\subsection{Proposizione: ricapitolazione sulle matrici invertibili}
Le seguenti sono equivalenti:
\begin{enumerate}
	\item $A\in\mathcal{M}_{m,n}(\mathbb{R})$ è invertibile (rispetto al prodotto di matrici)
	\item $L_A:\mathbb{R}^n\rightarrow\mathbb{R}^n$ è invertibile
	\item $L_A$ è iniettiva
	\item $L_A$ è suriettiva
	\item $KerA=\{0\}$
	\item $ImL_A=\mathbb{R}^n$ $(=ImA)$
	\item $rgA=n$
	\item Le colonne di $A$ sono indipendenti
	\item Le righe di $A$ sono indipendenti
	\item $AX=\underline{0}$ ha l'unica soluzione $X=\underline{0}$
	\item $\forall\underline{b}\in\mathbb{R}^n$ il sistema $AX=\underline{b}$ ammette unica soluzione (è $X=A^{-1}\cdot\underline{b}$)
	\item I pivot di una ridotta scala sono tutti non nulli
	\item $\exists\;B\in\mathcal{M}_n(\mathbb{R}):BA=I_n=\begin{bmatrix}1&\dots&0 \\&\dots&\\0&\dots&1\end{bmatrix}$ (\textit{tutti 1 sulla diagonale, resto 0}) [inversa a sinistra]
	\item $\exists\;C\in\mathcal{M}_n(\mathbb{R}):AC=I_n=\begin{bmatrix}1&\dots&0 \\&\dots&\\0&\dots&1\end{bmatrix}$ (\textit{tutti 1 sulla diagonale, resto 0})
	\item $[A|I]\sim[I|C]$ ($C^{-1}$ sarà l'inversa)
	\item $det(A)\neq 0$
\end{enumerate}

\subsection{Determinante di una matrice quadrata}
\[det: \mathcal{M}_n(\mathbb{R})\rightarrow\mathbb{R}\]
\[A\mapsto det(A)\;\text{ (è un numero)}\]

\subsubsection{Sistemi $AX=\underline{0}$}
$A$ è invertibile $\Leftrightarrow AX=\underline{0}$ ha unica soluzione.

\subsubsection{$n=1$}
$A=[a]\;a\in\mathbb{R};\;X=x;\;\underline{0}=0$.
\\$ax=0$ ha unica soluzione $\Leftrightarrow a\neq 0\Leftrightarrow a^{-1}ax=a^{-1}0$ ($x=0$)
\\(se $a=0: 0\cdot x=0$ ha infinite soluzioni).
Qui se pongo $det(a)=a$ ottengo:
\[[a]=A \text{ invertibile}\Leftrightarrow det(A)=a\neq 0\]
TODO: CONTROLLARE SUL LIBRO

\subsubsection{$n=2$}
$A$ invertibile $\Leftrightarrow AX=\underline{0}$ ha unica soluzione $\Leftrightarrow a_{11}a_{22}-a_{12}a_{22}\neq 0$.
\textit{Dimostrazione su appunti lezione 33.}
\\\textbf{Definisco:}
\[detA=det\begin{bmatrix}a_{11} & a_{22} \\ a_{21} & a_{22}=\end{bmatrix}=a_{11}a_{22}-a_{21}a_{22}\]

\subsubsection{$n=3$}
\[A=\begin{bmatrix}a_{11}& a_{12} & a_{13} \\ a_{21} & a_{22} & a_{23} \\ a_{31} & a_{32} & a_{33}\end{bmatrix}\]
\begin{center}
\begin{tabular}{c c | l| c}
  & $a_{11}\;a_{22}\;a_{33}+$ & $\begin{psmallmatrix}1&2&3\\1&2&3\end{psmallmatrix} = (1)(2)(3)$&segno pari\\
  &&&\\
  & $a_{12}\;a_{23}\;a_{31}+$ & $\begin{psmallmatrix}1&2&3\\2&3&1\end{psmallmatrix} = (123)$&$\varepsilon(\sigma)=1$\\
  &&&\\
  & $a_{13}\;a_{21}\;a_{32}+$ & $\begin{psmallmatrix}1&2&3\\3&1&2\end{psmallmatrix} = (132)$&\\\cline{3-4}
  det(A)= &&\\
  & $-a_{13}\;a_{22}\;a_{31}$ & $\begin{psmallmatrix}1&2&3\\3&2&1\end{psmallmatrix} = (13)$&segno dispari\\
  &&&\\
  & $-a_{11}\;a_{23}\;a_{32}$ & $\begin{psmallmatrix}1&2&3\\1&3&2\end{psmallmatrix} = (23)$&$\varepsilon(\sigma)=-1$\\
  &&&\\
  & $-a_{12}\;a_{21}\;a_{33}$ & $\begin{psmallmatrix}1&2&3\\2&1&3\end{psmallmatrix} = (12)$&\\
\end{tabular}
\end{center}

\subsubsection{Definizione determinante}
Data $A\in M_n(\mathbb{R})$, si definisce il determinante di $A$ come:
\[detA=\sum_{\sigma\in S_n}\varepsilon(\sigma)a_{1(\sigma 1)}a_{2(\sigma 2)}...a_{n(\sigma n)}\]
NOTA: Esempio di matrice di permutazionee su slides lezione 34.

\subsubsection{Proprietà del determinante}
\begin{itemize}
  \item $detA=detA^t$
  \item Se $A$ ha una colonna tutta nulla, allora $detA=0$ ($A^{(1)}\;A^{(2)}...\underline{0}...A^{(n)}$) colonne dipendenti.
  \item Se $A$ ha una riga nulla allora il $detA=0$ ($detA=detA^t$).
  \item Se due colonne di $A$ sono uguali, allora $detA=0$.
  \item Se dure righe di $A$ sono uguali, allora $detA=0$.
  \item Se due colonne (risp. righe) sono proporzionali, cioè se $A=[A^{(1)}...A^{(i)}...\lambda A^{(i)}...A^{(n)}]$ allora $detA=0$ ($\lambda A^{(i)}$ è la colonna $j$).
  \item Il valore del determinante non cambia se sommiamo ad una riga (vale anche l'analogo per le colonne) il multiplo di un'altra:
    \[A=
    \begin{bmatrix}
      A_{(1)}\\
      \vdots\\
      A_{(i)}\\
      \vdots
      A_{(n)}
    \end{bmatrix}
    \xrightarrow[L_{ij}(\lambda)]{}
    B=
    \begin{bmatrix}
      A_{(1)}\\
      \vdots\\
      A_{(i)}+A_{(j)}\\
      \vdots\\
      A_{(n)}\\
    \end{bmatrix}
    \]
    allora $det(A)=det(B)$.
  \item Se in $A$ si scambiano due colonne (o due righe)
\[
    A=
    \begin{bmatrix}
      A_{(1)}\\
      \vdots\\
      A_{(i)}\\
      \vdots\\
      A_{(j)}\\
      \vdots\\
      A_{(n)}
    \end{bmatrix}
    \xrightarrow[]{L_{ij}}
    B=
    \begin{bmatrix}
      A_{(1)}\\
      \vdots\\
      A_{(j)}\\
      \vdots\\
      A_{(i)}\\
      \vdots\\
      A_{(n)}\\
    \end{bmatrix}
\]
allora $detA=-detB$.
  \end{itemize}

  \subsubsection{Teorema di Binet}
  Siano $A,B$ matrici quadrate $n\times n$ allora:
  \[det(A\cdot B)=det(A)\cdot det(B)\]
  (cioè il determinante è una funzione moltiplicativa sulle matrici).

  \textit{!! non è additiva: $det(A+B)\neq det(A)+det(B)$}

  \subsubsection{Corollario di Binet}

  \[det(A^{-1})=\frac{1}{det(A)}\]
  se $A$ è invertibile.

  \subsubsection{Teorema di Lapalce}
  Calcolo del determinate col metodo di Laplace.

  Data $A$ $n\times n$, $n\geq 2$, si ha che (sviluppo lungo la colonna $j$ quindi $A^{(j)}$):
\[
  \begin{bmatrix}
    &&a_{1j}&&\\
    &&a_{2j}&&\\
    &&\vdots&&\\
    &&a_{nj}&&\\
  \end{bmatrix}
\]
\[detA= a_{1j}\alpha_{1j}+a_{2j}\alpha_{2j}+...+a_{nj}\alpha_{nj}\]
dove $\alpha_{ij}$ si chiama cofattore della matrice $A$, ed è definito come:
\[\alpha_{ij}(-1)^{i+j}\cdot detA_{ij}\]
$A_{ij}$: sottomatrice di $A$ che si ottiene cancellando la riga $i$ e la colonna $j$
\begin{enumerate}
  \item ovvero: (formula per la colonna $j$)
  \[det(A)=\sum_{i=1}^n(-1)^{i+j}a_{ij}det(A_{ij})\]
  $i$: varia l'indice di riga; $j$ è fisso.
\item formula (sviluppo) lungo la riga $i$
  \[det(A)=\sum_{j=1}^n(-1)^{i+j}a_{ij}det(A_{ij})\]
  $j$: varia l'indice di colonna; $i$ è fisso.

\end{enumerate}
NOTA: vedere appunti lezione 14 per esempio.

\subsubsection{Metodo alternativo calcolo determinante}
$A\sim B$ usando solo $L_{ik},L_{ij}(C)$
\\$A\xrightarrow[]{L_{ij}}A'$: $detA=-detA'$ ($(-1)\cdot$ numero di scambi di riga)
\\$A\xrightarrow[]{L_{ij}(C)}A'$: $detA=detA'$
\\Non usare $L_i(C)$ perche modifica il determinante.
\[
  A=
\begin{bmatrix}
  &&&\\
  &&&\\
  &&&\\
  &&&
\end{bmatrix}
\sim B=
\begin{bmatrix}
  b_{11}&&&\\
  &b_{22}&&\\
  &&b_{33}&\\
  &&&b_{44}
\end{bmatrix}
\]
\\Il $detB=$ prodotto degli elementi sulla diagonale.

\textbf{Proprietà}: Se una matrice $B$ quadrata è triangolare superiore (cioè $b_{ij}=0$ se $i>j$)
\[
  \begin{bmatrix}
    b_{11} & b_{12} & \dots& b_{1n}\\
    0 & b_{22} & \dots & b_{2n}\\
    0 &0 & \dots \\
    0 & \dots & 0 & b_{nn}
    
  \end{bmatrix}
\]
allora $detB=b_{11}b_{22}...b_{nn}$ (prodotto degli elementi sulla diagonale.
\begin{itemize}
\item In particolare se $b$ è una matrice diagonale, ovvero $b_{ij}=0$ se $i\neq j$, allora $detA=b_{11}b_{22}...b_{nn}$
    \item In particolare se
  \[
    B=
    \begin{bmatrix}
      \lambda&&\text{\Huge0}\\
      &\lambda&&\\
      &\dots&&\\
      \text{\Huge0}&&\lambda
    \end{bmatrix}
    \text{ per }\lambda\in\mathbb{R}
    \]
    Allora $detB=\lambda^n$.

  \item $detI_n=
    \begin{bmatrix}
      1 &&\text{\Huge0}\\
      &\dots&\\
      \text{\Huge0}&&1
    \end{bmatrix}
=1$
(Se a matrice ha tutti $1$ sulla diagonale).

  \item Se $P$ è una matrice di permutazione, allora $detP=\varepsilon(\sigma)$ se $P=P_\sigma$: $P_\sigma$ è la matrice che ha $1$ in posizione $a_{ij}$ se $ j=\sigma(i)$
NOTA: Esempio appunti lezione 34.
\end{itemize}

\subsection{Cambiamento di base}
\begin{itemize}
    \item Come mutano le coordinate dei vettori nelle due basi diverse.
    \item Come mutano le trasformazioni lineari.
    \item Come cambiano le matrici associate alle trasformazioni quando cambiano le basi.
\end{itemize}


Siano $\mathcal{B}=(v_1,...,v_n)$ e $\mathcal{B}'=(v'_1,...,v'_n)$ due basi ordinate di uno spazio vettoriale $V$ (su $\mathbb{R}$ o su $K$ fissato) di dimensione $n$ su $\mathbb{R}$ (su $K$...).
Siano $f_{\mathcal{B}}$ ed $f_{\mathcal{B}'}$ gli isomorfismi che mandano i vettori di $V$ nelle coordinate (colonne di $\mathbb{R}^n$) rispettivamente nella base $\mathcal{B}$ e $\mathcal{B}'$.

\begin{center}
  \includegraphics[scale=0.5]{cambiamento_di_base}
\end{center}
\[id_V: V\rightarrow V\]
\[v\mapsto v=id_V(v)\]
($id_V$ è una trasformazione lineare biunivoca) è l'elemento neutro nella composizione delle trasf.lineari da $V$ in se.

\textbf{Richiamo} Se $\mathcal{B}$ è una base, allora l'applicazione
\[f_{\mathcal{B}}: V\rightarrow \mathbb{R}\]
\[v\rightarrow \begin{bmatrix}x_1\\\vdots\\x_n\end{bmatrix}\]
Se $v=x_1v_1+...+x_nv_n$, $\begin{bmatrix}x_1\\\vdots\\x_n\end{bmatrix}=f_{\mathcal{B}}(v)$ è:
\begin{itemize}
  \item Lineare
  \item Biunivoca
\end{itemize}

\begin{center}
  \includegraphics[scale=0.5]{cambiamento_base_2}
\end{center}
\[v=x'_1v'_1+x'_2v'_2+x'_3v'_3=f_{\mathcal{B}'}^{-1}\begin{bmatrix}x_1\\x_2\\x_3\end{bmatrix}\]
\[f_{\mathcal{B}}\circ id_V\circ f_{\mathcal{B}'}^{-1}\]
Siano:

$f_{\mathcal{B}}(v)=\begin{bmatrix}x_1\\\vdots\\x_n\end{bmatrix}$ le coordinate di $v$ in base $\mathcal{B}$

$f_{\mathcal{B}'}(v)=\begin{bmatrix}x'_1\\\vdots\\x'_n\end{bmatrix}$ le coordinate di $v$ in base $\mathcal{B}'$

Allora esiste un'unica applicazione $\Psi$ che rende il diagramma commutativo:


